# running locally.

In order to run locally, you need a copy of [tornado](http://tornadoweb.org/) available in your import path. To upload to appengine, you must also have it as a local directory.

	git clone https://github.com/facebook/tornado.git tmp_clone
	mv tmp_clone/tornado tornado
	rm -rf tmp_clone

# bulk load a copy of production agency data. (note: you probably want to run the appengine with --datastore_path=... so data is not stored persistently in '/tmp'. Also make sure the appengine runtime is running in python 2.5)

To dump the dev datastore startup with --clear-datastore. Note: it's important to run these with python 2.5

    cd data
    for kind in Counter Agency MessageAgency AgencyAlias CrawlBaseUrl CrawlSkipUrl Message; do
        python2.5 /usr/local/bin/appcfg.py download_data --application=gtfs-data-exchange --kind=$kind --url=http://gtfs-data-exchange.appspot.com/remote_api --filename=$kind
    done

    cd data
    for kind in Counter Agency MessageAgency AgencyAlias CrawlBaseUrl CrawlSkipUrl Message; do
        python2.5 /usr/local/bin/appcfg.py upload_data --application=dev~gtfs-data-exchange --kind=$kind --url=http://127.0.0.1:8087/remote_api --filename=$kind --email=you@domain.com
    done

    rm bulkloader-*

# load a copy of agency data only
    
    note: this deletes existing data first, but does not require admin access to gtfs-data-exchange

    $ cd scripts
    $ python bulk_load.py

# crawl a file and queue it for upload

    $ cd contrib
    $ python gtfs_crawler.py --crawl_url=http://www.yourdomain.com/path/to/google_transit.zip --token="asdf"

# process the zip file

    $ cd contrib
    $ python background_processor.py --token="asdf"

# New Data Workflow

GTFS File uploads follow an odd chain of events

a) gtfs_crawler.py queries /crawl/next_url and fetches the next url to crawl (and metadata)
a) gtfs_crawler.py recursively crawls and sends any zip files to S3 with a name of queue/{{timestamp}} (and attaching metadata)
b) user uploads directly to S3 w/ name of queue/{{timestamp}} and some metadata
* background_processor.py queries S3 for entries starting with queue/, downloads/extracts the s3 metadata, content and validates it as a gtfs file
* background_processor.py uploads metadata to /crawl/upload and gets back the name of the file
* /crawl/upload makes a Message, and a MessageAgency linking to all agencies in the agency.txt file (creating agencies as needed)
* background_processor.py renames the file on S3, and sends an email

