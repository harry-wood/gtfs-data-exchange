
# bulk load a copy of production agency data

To dump the dev datastore startup with --clear-datastore

    cd data
    for kind in Counter Agency MessageAgency AgencyAlias CrawlBaseUrl CrawlSkipUrl Message; do
        appcfg.py download_data --application=gtfs-data-exchange --kind=$kind --url=http://gtfs-data-exchange.appspot.com/remote_api --filename=$kind
    done

    cd data
    for kind in Counter Agency MessageAgency AgencyAlias CrawlBaseUrl CrawlSkipUrl Message; do
        appcfg.py upload_data --application=gtfs-data-exchange --kind=$kind --url=http://127.0.0.1:8087/remote_api --filename=$kind
    done

    rm bulkloader-*

# note: this deletes existing data first, but does not require admin access to gtfs-data-exchange

    $ python bulk_load.py

# crawl a file and queue it for upload

    $ cd contrib
    $ python gtfs_crawler.py --allow-upload --shunt http://www.yourdomain.com/path/to/google_transit.zip

# process the zip file

    $ cd contrib
    $ python BackgroundProcessor.py

# New Data Workflow

GTFS File uploads follow an odd chain of events

a) gtfs_crawler.py queries /crawl/next_url and fetches the next url to crawl (and metadata)
a) gtfs_crawler.py recursively crawls and sends any zip files to S3 with a name of queue/{{timestamp}} (and attaching metadata)
b) user uploads directly to S3 w/ name of queue/{{timestamp}} and some metadata
* BackgroundProcessor.py queries S3 for entries starting with queue/, downloads/extracts the s3 metadata, content and validates it as a gtfs file
* BackgroundProcessor.py uploads content and s3 metadata to /crawl/upload and gets back the name of the file
* /crawl/upload makes a Message, and a MessageAgency linking to all agencies in the agency.txt file (creating agencies as needed)
* BackgroundProcessor.py renames the file on S3, and sends an email

